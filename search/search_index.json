{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Crawling Web \u00b6 Crawling merupakan proses untuk mendapatkan suatu data dari suatu web. Berikut Merupakan contoh code crawling data text dari web. Sebelum menjalakan program harus menginstall scrapy di cmd dengan kode seperti dibawah ini. pip install scrapy setelah itu, masuk ke direktori kemudian jalankan kode berikut untuk membuat projek baru. scrapy startproject crawling setelah folder selesai. membuat import scrapy seperti berilut : import scrapy Selanjutnya membuat class dan fumgsi untuk url yang akan di crawling, dan masukan link url seperti berikut : class QuotesSpider ( scrapy . Spider ): name = \"quotes\" def start_requests ( self ): urls = [ \"https://pta.trunojoyo.ac.id/welcome/detail/080211100070\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211200001\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100044\" , \"https://pta.trunojoyo.ac.id/welcome/detail/100211200002\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100119\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100103\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100098\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100079\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100089\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100013\" , ] Membuat def parse untuk kolom yang akan di crawling. seperti gambar berikut: def parse(self, response): yield{ 'Abstrak' : response.css('#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > ::text').extract(), } kode lengkap serperti berikut : import scrapy class QuotesSpider ( scrapy . Spider ): name = \"quotes\" def start_requests ( self ): urls = [ \"https://pta.trunojoyo.ac.id/welcome/detail/080211100070\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211200001\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100044\" , \"https://pta.trunojoyo.ac.id/welcome/detail/100211200002\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100119\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100103\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100098\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100079\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100089\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100013\" , ] for url in urls : yield scrapy . Request ( url = url , callback = self . parse ) def parse ( self , response ): yield { 'Abstrak' : response . css ( '#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > ::text' ) . extract (), } Untuk menjalankan file buka cmd di lokasi file kemudian jalankan kode berikut : scrapy run spider crawling.py Untuk menyimpan data dalam bentuk csv tinnggal menambakan (-O nama file).csv di cmd seperti berikut: scrapy run spider pta.py -crawling.csv Latent Semantic Analysis \u00b6 Latent Semantic Analysis merupakan algoritma yang digunakan untuk menganalisa hubungan antara sebuah frase atau kalimat pada sebuah dokumen. Sebelum menganalisa, data harus di TF-IDF terlebih dahulu. Berikut merupakan langkah-langkah untuk menganalisa data menggunakan metode Latent Semantic Analysis. Membuat import seperti berikut : import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import style import seaborn as sns % matplotlib inline style . use ( 'fivethirtyeight' ) sns . set ( style = 'whitegrid' , color_codes = True ) import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize , sent_tokenize #preprosessing from nltk.corpus import stopwords #stopwords from nltk import word_tokenize , sent_tokenize # tokenizing from nltk.stem import PorterStemmer , LancasterStemmer # using the Porter Stemmer and Lancaster Stemmer and others from nltk.stem.snowball import SnowballStemmer from nltk.stem import WordNetLemmatizer # lammatizer from WordNet # for named entity recognition (NER) from nltk import ne_chunk # vectorizers untuk membuat document-term-matrix (DTM) from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer #stop-words stop_words = set ( nltk . corpus . stopwords . words ( 'indonesian' )) Kemudian memanggil data yang akan di publish. kemudian run df=pd.read_csv('ekonomi_management.csv') df.head() jika berhasil akan muncul data seperti berikut : Aplikasi nyata pemanfaatan teknologi informasi... 1 Abstrak\\r\\nPenelitian ini menggunakan metode k... 2 ABSTRAK\\r\\n\\tDharma Abidin Syah,Kesimpulan: (1... 3 ABSTRAK\\r\\... 4 Tujuan penelitian ini adalah untuk mengetahui ... Kemudian melakukan pre-processing data dan proses cleaning pada data import string import re #regex library # import word_tokenize & FreqDist from NLTK from nltk.tokenize import word_tokenize from nltk.probability import FreqDist #remove number def remove_number ( text ): return re . sub ( r \"\\d+\" , \"\" , text ) df [ 'hapus angka' ] = df [ 'Abstrak' ] . apply ( remove_number ) df . head ( 10 ) Menghapus simbol dan tanda baca def remove_punctuation ( text ) : return text . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) df [ ' hapus simbol ' ] = df [ ' hapus angka ' ]. apply ( remove_punctuation ) df . head ( 10 ) def clean_text ( headline ) : le = WordNetLemmatizer () word_tokens = word_tokenize ( headline ) tokens = [ le . lemmatize ( w ) for w in word_tokens if w not in stop_words and len ( w ) > 3 ] cleaned_text = \" \" . join ( tokens ) return cleaned_text df [ ' stopword ' ] = df [ ' hapus simbol ' ]. apply ( clean_text ) df . head ( 10 ) df.drop(['Abstrak', 'hapus angka', 'hapus simbol'],axis=1,inplace=True) df.head(10) df['stopword'][0] vect =TfidfVectorizer(stop_words=stop_words,max_features=1000) vect_text=vect.fit_transform(df['stopword']) print(vect_text.shape) print(vect_text) IDF idf=vect.idf_ dd=dict(zip(vect.get_feature_names(), idf)) l=sorted(dd, key=(dd).get) print(l[0],l[-1]) Latent Semantic Analysis from sklearn.decomposition import TruncatedSVD lsa_model = TruncatedSVD ( n_components = 10 , algorithm = 'randomized' , n_iter = 10 , random_state = 42 ) lsa_top = lsa_model . fit_transform ( vect_text ) print ( lsa_top ) print ( lsa_top . shape ) # (no_of_doc*no_of_topics) l = lsa_top [ 0 ] print ( \"Document 0 :\" ) for i , topic in enumerate ( l ): print ( \"Topic \" , i , \" : \" , topic * 100 ) print(lsa_model.components_.shape) # (no_of_topics*no_of_words) print(lsa_model.components_) Menampilkan kata yang penting di setiap topik. vocab = vect . get_feature_names () for i , comp in enumerate ( lsa_model . components_ ): vocab_comp = zip ( vocab , comp ) sorted_words = sorted ( vocab_comp , key = lambda x : x [ 1 ], reverse = True )[: 15 ] print ( \"Topic \" + str ( i ) + \": \" ) for t in sorted_words : print ( t [ 0 ], end = \" \" ) print ( \" \\n \" )","title":"UAS Pengembangan dan Pencarian Web"},{"location":"#crawling_web","text":"Crawling merupakan proses untuk mendapatkan suatu data dari suatu web. Berikut Merupakan contoh code crawling data text dari web. Sebelum menjalakan program harus menginstall scrapy di cmd dengan kode seperti dibawah ini. pip install scrapy setelah itu, masuk ke direktori kemudian jalankan kode berikut untuk membuat projek baru. scrapy startproject crawling setelah folder selesai. membuat import scrapy seperti berilut : import scrapy Selanjutnya membuat class dan fumgsi untuk url yang akan di crawling, dan masukan link url seperti berikut : class QuotesSpider ( scrapy . Spider ): name = \"quotes\" def start_requests ( self ): urls = [ \"https://pta.trunojoyo.ac.id/welcome/detail/080211100070\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211200001\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100044\" , \"https://pta.trunojoyo.ac.id/welcome/detail/100211200002\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100119\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100103\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100098\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100079\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100089\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100013\" , ] Membuat def parse untuk kolom yang akan di crawling. seperti gambar berikut: def parse(self, response): yield{ 'Abstrak' : response.css('#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > ::text').extract(), } kode lengkap serperti berikut : import scrapy class QuotesSpider ( scrapy . Spider ): name = \"quotes\" def start_requests ( self ): urls = [ \"https://pta.trunojoyo.ac.id/welcome/detail/080211100070\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211200001\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100044\" , \"https://pta.trunojoyo.ac.id/welcome/detail/100211200002\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100119\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100103\" , \"https://pta.trunojoyo.ac.id/welcome/detail/080211100098\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100079\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100089\" , \"https://pta.trunojoyo.ac.id/welcome/detail/090211100013\" , ] for url in urls : yield scrapy . Request ( url = url , callback = self . parse ) def parse ( self , response ): yield { 'Abstrak' : response . css ( '#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > ::text' ) . extract (), } Untuk menjalankan file buka cmd di lokasi file kemudian jalankan kode berikut : scrapy run spider crawling.py Untuk menyimpan data dalam bentuk csv tinnggal menambakan (-O nama file).csv di cmd seperti berikut: scrapy run spider pta.py -crawling.csv","title":"Crawling Web"},{"location":"#latent_semantic_analysis","text":"Latent Semantic Analysis merupakan algoritma yang digunakan untuk menganalisa hubungan antara sebuah frase atau kalimat pada sebuah dokumen. Sebelum menganalisa, data harus di TF-IDF terlebih dahulu. Berikut merupakan langkah-langkah untuk menganalisa data menggunakan metode Latent Semantic Analysis. Membuat import seperti berikut : import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import style import seaborn as sns % matplotlib inline style . use ( 'fivethirtyeight' ) sns . set ( style = 'whitegrid' , color_codes = True ) import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize , sent_tokenize #preprosessing from nltk.corpus import stopwords #stopwords from nltk import word_tokenize , sent_tokenize # tokenizing from nltk.stem import PorterStemmer , LancasterStemmer # using the Porter Stemmer and Lancaster Stemmer and others from nltk.stem.snowball import SnowballStemmer from nltk.stem import WordNetLemmatizer # lammatizer from WordNet # for named entity recognition (NER) from nltk import ne_chunk # vectorizers untuk membuat document-term-matrix (DTM) from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer #stop-words stop_words = set ( nltk . corpus . stopwords . words ( 'indonesian' )) Kemudian memanggil data yang akan di publish. kemudian run df=pd.read_csv('ekonomi_management.csv') df.head() jika berhasil akan muncul data seperti berikut : Aplikasi nyata pemanfaatan teknologi informasi... 1 Abstrak\\r\\nPenelitian ini menggunakan metode k... 2 ABSTRAK\\r\\n\\tDharma Abidin Syah,Kesimpulan: (1... 3 ABSTRAK\\r\\... 4 Tujuan penelitian ini adalah untuk mengetahui ... Kemudian melakukan pre-processing data dan proses cleaning pada data import string import re #regex library # import word_tokenize & FreqDist from NLTK from nltk.tokenize import word_tokenize from nltk.probability import FreqDist #remove number def remove_number ( text ): return re . sub ( r \"\\d+\" , \"\" , text ) df [ 'hapus angka' ] = df [ 'Abstrak' ] . apply ( remove_number ) df . head ( 10 ) Menghapus simbol dan tanda baca def remove_punctuation ( text ) : return text . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) df [ ' hapus simbol ' ] = df [ ' hapus angka ' ]. apply ( remove_punctuation ) df . head ( 10 ) def clean_text ( headline ) : le = WordNetLemmatizer () word_tokens = word_tokenize ( headline ) tokens = [ le . lemmatize ( w ) for w in word_tokens if w not in stop_words and len ( w ) > 3 ] cleaned_text = \" \" . join ( tokens ) return cleaned_text df [ ' stopword ' ] = df [ ' hapus simbol ' ]. apply ( clean_text ) df . head ( 10 ) df.drop(['Abstrak', 'hapus angka', 'hapus simbol'],axis=1,inplace=True) df.head(10) df['stopword'][0] vect =TfidfVectorizer(stop_words=stop_words,max_features=1000) vect_text=vect.fit_transform(df['stopword']) print(vect_text.shape) print(vect_text) IDF idf=vect.idf_ dd=dict(zip(vect.get_feature_names(), idf)) l=sorted(dd, key=(dd).get) print(l[0],l[-1]) Latent Semantic Analysis from sklearn.decomposition import TruncatedSVD lsa_model = TruncatedSVD ( n_components = 10 , algorithm = 'randomized' , n_iter = 10 , random_state = 42 ) lsa_top = lsa_model . fit_transform ( vect_text ) print ( lsa_top ) print ( lsa_top . shape ) # (no_of_doc*no_of_topics) l = lsa_top [ 0 ] print ( \"Document 0 :\" ) for i , topic in enumerate ( l ): print ( \"Topic \" , i , \" : \" , topic * 100 ) print(lsa_model.components_.shape) # (no_of_topics*no_of_words) print(lsa_model.components_) Menampilkan kata yang penting di setiap topik. vocab = vect . get_feature_names () for i , comp in enumerate ( lsa_model . components_ ): vocab_comp = zip ( vocab , comp ) sorted_words = sorted ( vocab_comp , key = lambda x : x [ 1 ], reverse = True )[: 15 ] print ( \"Topic \" + str ( i ) + \": \" ) for t in sorted_words : print ( t [ 0 ], end = \" \" ) print ( \" \\n \" )","title":"Latent Semantic Analysis"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY Support Author \u00b6 Amazon wish list","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#support_author","text":"Amazon wish list","title":"Support Author"},{"location":"material-for-mkdocs/","text":"Material for MkDocs \u00b6 MkDocs \u00b6 mkdocs/mkdocs: Project documentation with Markdown - GitHub Material for MkDocs \u00b6 squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs","text":"","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#mkdocs","text":"mkdocs/mkdocs: Project documentation with Markdown - GitHub","title":"MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs_1","text":"squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure_mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure_mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure_mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example_code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example_rendering","text":"P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"Example rendering"},{"location":"getting-started/docker/","text":"Start with Docker \u00b6 Public docker image \u00b6 Package mkdocs-material - GitHub peaceiris/mkdocs-material - Docker Hub docker-compose \u00b6 Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"Start with Docker"},{"location":"getting-started/docker/#start_with_docker","text":"","title":"Start with Docker"},{"location":"getting-started/docker/#public_docker_image","text":"Package mkdocs-material - GitHub peaceiris/mkdocs-material - Docker Hub","title":"Public docker image"},{"location":"getting-started/docker/#docker-compose","text":"Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"docker-compose"},{"location":"getting-started/download-boilerplate/","text":"Download boilerplate \u00b6 Git clone \u00b6 git clone https://github.com/peaceiris/mkdocs-material-boilerplate.git cd mkdocs-material-boilerplate Download zip \u00b6 wget 'https://github.com/peaceiris/mkdocs-material-boilerplate/archive/master.zip' unzip master.zip cd mkdocs-material-boilerplate-master \ud83d\udc49 Click me to download zip","title":"Download boilerplate"},{"location":"getting-started/download-boilerplate/#download_boilerplate","text":"","title":"Download boilerplate"},{"location":"getting-started/download-boilerplate/#git_clone","text":"git clone https://github.com/peaceiris/mkdocs-material-boilerplate.git cd mkdocs-material-boilerplate","title":"Git clone"},{"location":"getting-started/download-boilerplate/#download_zip","text":"wget 'https://github.com/peaceiris/mkdocs-material-boilerplate/archive/master.zip' unzip master.zip cd mkdocs-material-boilerplate-master \ud83d\udc49 Click me to download zip","title":"Download zip"},{"location":"getting-started/invoke/","text":"Serve and open with invoke \u00b6 invoke \u00b6 Invoke is a Python (2.7 and 3.4+) library for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. It draws inspiration from various sources (make/rake, Fabric 1.x, etc) to arrive at a powerful & clean feature set. pyinvoke/invoke: Pythonic task management & command execution. Serve and open \u00b6 Run mkdocs serve and open browser automatically. inv serve Serving on localhost:8000 # set IP and port inv serve --dev-addr 'localhost:5000' # set config file inv serve --config-file ./mkdocs-sample.yml Show all tasks \u00b6 $ inv --list Available tasks: serve Serve site and open browser Show task help. $ inv serve --help Usage: inv [ oke ] [ --core-opts ] serve [ --options ] [ other tasks here ... ] Docstring: Serve site and open browser Options: -c STRING, --config-file = STRING Provide a specific MkDocs config -d STRING, --dev-addr = STRING IP address and port to serve documentation locally ( default: localhost:8000 ) Tasks are defined by tasks.py","title":"Serve and open with invoke"},{"location":"getting-started/invoke/#serve_and_open_with_invoke","text":"","title":"Serve and open with invoke"},{"location":"getting-started/invoke/#invoke","text":"Invoke is a Python (2.7 and 3.4+) library for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. It draws inspiration from various sources (make/rake, Fabric 1.x, etc) to arrive at a powerful & clean feature set. pyinvoke/invoke: Pythonic task management & command execution.","title":"invoke"},{"location":"getting-started/invoke/#serve_and_open","text":"Run mkdocs serve and open browser automatically. inv serve Serving on localhost:8000 # set IP and port inv serve --dev-addr 'localhost:5000' # set config file inv serve --config-file ./mkdocs-sample.yml","title":"Serve and open"},{"location":"getting-started/invoke/#show_all_tasks","text":"$ inv --list Available tasks: serve Serve site and open browser Show task help. $ inv serve --help Usage: inv [ oke ] [ --core-opts ] serve [ --options ] [ other tasks here ... ] Docstring: Serve site and open browser Options: -c STRING, --config-file = STRING Provide a specific MkDocs config -d STRING, --dev-addr = STRING IP address and port to serve documentation locally ( default: localhost:8000 ) Tasks are defined by tasks.py","title":"Show all tasks"},{"location":"getting-started/pip/","text":"Start with pip (Anaconda, Miniconda) \u00b6 pip install -r requirements.txt pip install -r requirements-dev.txt inv command is also available.","title":"Start with pip (Anaconda, Miniconda)"},{"location":"getting-started/pip/#start_with_pip_anaconda_miniconda","text":"pip install -r requirements.txt pip install -r requirements-dev.txt inv command is also available.","title":"Start with pip (Anaconda, Miniconda)"},{"location":"getting-started/pipenv/","text":"Start with pipenv \u00b6 pipenv \u00b6 Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. pypa/pipenv: Python Development Workflow for Humans. Install all packages \u00b6 pipenv sync --dev # Installs all packages specified in Pipfile.lock. Run MkDocs \u00b6 pipenv shell # Spawns a shell within the virtualenv. mkdocs serve Or, run mkdocs with pipenv run pipenv run mkdocs serve pipenv run \u00b6 pipenv task are also defined by Pipfile pipenv run version # mkdocs --version pipenv run help # mkdocs --help pipenv run inv serve # inv serve pipenv run serve # mkdocs serve pipenv run build # mkdocs build pipenv run deploy # mkdocs gh-deploy","title":"Start with pipenv"},{"location":"getting-started/pipenv/#start_with_pipenv","text":"","title":"Start with pipenv"},{"location":"getting-started/pipenv/#pipenv","text":"Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. pypa/pipenv: Python Development Workflow for Humans.","title":"pipenv"},{"location":"getting-started/pipenv/#install_all_packages","text":"pipenv sync --dev # Installs all packages specified in Pipfile.lock.","title":"Install all packages"},{"location":"getting-started/pipenv/#run_mkdocs","text":"pipenv shell # Spawns a shell within the virtualenv. mkdocs serve Or, run mkdocs with pipenv run pipenv run mkdocs serve","title":"Run MkDocs"},{"location":"getting-started/pipenv/#pipenv_run","text":"pipenv task are also defined by Pipfile pipenv run version # mkdocs --version pipenv run help # mkdocs --help pipenv run inv serve # inv serve pipenv run serve # mkdocs serve pipenv run build # mkdocs build pipenv run deploy # mkdocs gh-deploy","title":"pipenv run"},{"location":"hosting-and-deployment/aws-amplify-console/","text":"Host on AWS Amplify Console \u00b6 AWS Amplify Console You can use Password protection each branch. Use the following build specification YML file. mkdocs-material-boilerplate/amplify.yml","title":"Host on AWS Amplify Console"},{"location":"hosting-and-deployment/aws-amplify-console/#host_on_aws_amplify_console","text":"AWS Amplify Console You can use Password protection each branch. Use the following build specification YML file. mkdocs-material-boilerplate/amplify.yml","title":"Host on AWS Amplify Console"},{"location":"hosting-and-deployment/combinations/","text":"Hosting and Deployment \u00b6 GitHub Pages and GitHub \u00b6 Host source code on GitHub. Build and deploy with: mkdocs gh-deploy GitHub Actions GitLab Pages and GitLab \u00b6 Host source code on GitLab. Build and deploy with GitLab CI/CD. Netlify \u00b6 Host source code on: GitHub GitLab BitBucket Build and deploy with Netlify. AWS Amplify Console \u00b6 Host source code on: GitHub GitLab BitBucket AWS CodeCommit Build and deploy with AWS Amplify Console.","title":"Hosting and Deployment"},{"location":"hosting-and-deployment/combinations/#hosting_and_deployment","text":"","title":"Hosting and Deployment"},{"location":"hosting-and-deployment/combinations/#github_pages_and_github","text":"Host source code on GitHub. Build and deploy with: mkdocs gh-deploy GitHub Actions","title":"GitHub Pages and GitHub"},{"location":"hosting-and-deployment/combinations/#gitlab_pages_and_gitlab","text":"Host source code on GitLab. Build and deploy with GitLab CI/CD.","title":"GitLab Pages and GitLab"},{"location":"hosting-and-deployment/combinations/#netlify","text":"Host source code on: GitHub GitLab BitBucket Build and deploy with Netlify.","title":"Netlify"},{"location":"hosting-and-deployment/combinations/#aws_amplify_console","text":"Host source code on: GitHub GitLab BitBucket AWS CodeCommit Build and deploy with AWS Amplify Console.","title":"AWS Amplify Console"},{"location":"hosting-and-deployment/github-pages/","text":"Host on GitHub Pages \u00b6 Demo site on GitHub Pages (build & deploy with GitHub Actions) Build and deploy with GitHub Actions \u00b6 peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details. Build and deploy with mkdocs gh-deploy \u00b6 pipenv \u00b6 pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"Host on GitHub Pages"},{"location":"hosting-and-deployment/github-pages/#host_on_github_pages","text":"Demo site on GitHub Pages (build & deploy with GitHub Actions)","title":"Host on GitHub Pages"},{"location":"hosting-and-deployment/github-pages/#build_and_deploy_with_github_actions","text":"peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details.","title":"Build and deploy with GitHub Actions"},{"location":"hosting-and-deployment/github-pages/#build_and_deploy_with_mkdocs_gh-deploy","text":"","title":"Build and deploy with mkdocs gh-deploy"},{"location":"hosting-and-deployment/github-pages/#pipenv","text":"pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"pipenv"},{"location":"hosting-and-deployment/gitlab-pages/","text":"Host on GitLab Pages \u00b6 See .gitlab-ci.yml","title":"Host on GitLab Pages"},{"location":"hosting-and-deployment/gitlab-pages/#host_on_gitlab_pages","text":"See .gitlab-ci.yml","title":"Host on GitLab Pages"},{"location":"hosting-and-deployment/netlify/","text":"Host on Netlify \u00b6 Demo site on Netlify (build & deploy with Netlify) Create GitHub repository and deploy to Netlify with the following button in 1 min.","title":"Host on Netlify"},{"location":"hosting-and-deployment/netlify/#host_on_netlify","text":"Demo site on Netlify (build & deploy with Netlify) Create GitHub repository and deploy to Netlify with the following button in 1 min.","title":"Host on Netlify"}]}